# Crazy Challenge: Inference Engine Debugging

## 项目简介

欢迎参加本次挑战！本项目旨在 CPU 上运行大型语言模型推理引擎的修改版本。

**任务：** 对提供的代码（ 主要涉及`main.cpp`, `utils.cpp`, `attention.cpp` 等文件）进行诊断并修复，使其能够成功加载提供的语言模型文件，并根据用户输入的提示（prompt）生成连贯的文本输出。

**注意：** 你收到的代码库中被植入了一些bug，导致项目出现**编译错误**和**程序无法按预期工作**。你需要运用你的 C++ 编程和调试技能来定位并解决这些问题。

## 构建指南

使用 CMake 构建项目：

1.  **创建构建目录:**
    ```bash
    mkdir build
    cd build
    ```

2.  **运行 CMake 配置:**
    ```bash
    cmake ..
    ```
    * 如果遇到问题，你可能需要修改 `CMakeLists.txt` 文件。 

3.  **编译项目:**
    ```bash
    make -j
    ```
    * `-j` 参数可以并行编译以加快速度。这一过程可能会出错，你需要找出问题并解决。

成功编译后，你应该能在 `build/` 目录下找到名为 `llama` (或者 `llama.exe`) 的可执行文件。

## 运行与测试

要运行推理引擎代码，请使用 `llama` 可执行文件，并至少提供模型文件的路径。

**运行步骤:**

1. 使用 huggingface-cli 从`https://huggingface.co/DengCaptain/Llama-7b`上下载Llama-7B的模型权重文件到models-hf中（如果下载过程中显示没有权限，你需要在huggingface仓库上同意仓库的规则）。
2. 或者，从百度云盘下载模型权重:通过网盘分享的文件：链接: https://pan.baidu.com/s/1GM5FpN6yCry1_LLCOWDPeQ?pwd=u5hr 提取码: u5hr
3. 使用 `conver-pth-to-dragon.py` 脚本将models-hf中的pytorch格式的模型文件转为本项目的格式，比如dragon-model-f16.bin。
4. 可选：为了减轻内存负载，使用quantize.cpp将模型量化为int4格式。
5. 以上一步骤生成的模型文件作为输入参数，运行 `llama` 可执行文件。这一步你可能会遇到若干个错误，你需要找出原因并解决这些错误。
6. 当程序最终可以生成具有正确语义的token序列时（如下图），你就已经解决了本任务了。
![最后运行结果展示](figs/result.png)

## 调试提示

为了降低调试难度，下面列出部分bug的类型和数量：

1. 网络结构bug：2个
2. 线程同步bug：1个（你可能要修改多行代码来修复这个bug）

operators.h 文件的43行的 dragon_op 枚举了本项目需要用到的算子。

## 提交指南

在成功修复代码并能正确运行程序之后，你需要提交一份解题报告，内容应至少包括：

1. **Bug 定位：** 清晰地说明你找到了哪些 bugs，并指出每个 bug 所在的具体文件和代码行号（修复前的行号）。
2. **Bug 分析：** 详细解释每个 bug 产生的原因。例如，是逻辑错误、变量使用不当、状态管理问题、文件格式不匹配，还是其他原因？
3. **Bug 修复：** 描述你对每个 bug 采取的具体修复措施。可以附上修改后的关键代码片段。
4. **所花时间：** 我们希望你能如实填写你解决每一个bug所花费的时间。
5. **git记录：** 在做题过程中，你应该是使用git进行版本管理。最后将包括.git的源代码文件夹打包提交。
6. **最终结果：** 提供你用于测试的最终运行命令，并附上一段程序成功运行后截图，以证明你的修复是有效的。
7. **提示：** **不管你最后是否得到了预期结果**，都应该及时提交解体报告。我们会根据你的阶梯报告对你进行评估。

---

时间限制：**12小时**。祝您好运！
